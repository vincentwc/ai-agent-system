from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_models.tongyi import ChatTongyi
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda

model = ChatTongyi(model="qwen3-max")
str_parser = StrOutputParser()

first_prompt = PromptTemplate.from_template(
    "我的领居姓：{lastname},刚生了{gender},请帮我起一个名字,仅告知我名字，不要额外信息"
)

second_prompt = PromptTemplate.from_template("姓名:{name},请帮我解释含义")

# 函数的入参：AIMessage -》 dict
my_func = RunnableLambda(lambda ai_msg: {"name": ai_msg.content})

# chain = first_prompt | model | my_func | second_prompt | model | str_parser

chain = (
    first_prompt
    | model
    | (lambda ai_msg: {"name": ai_msg.content})
    | second_prompt
    | model
    | str_parser
)

for chunk in chain.stream({"lastname": "王", "gender": "男"}):
    print(chunk, end="", flush=True)
